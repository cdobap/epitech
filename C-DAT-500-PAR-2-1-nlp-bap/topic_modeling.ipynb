{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e7a8e5",
   "metadata": {},
   "source": [
    "PROJET : TOPIC MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3ac79",
   "metadata": {},
   "source": [
    "Dépendences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f4fbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In case of import errors\n",
    "# !pip install nltk\n",
    "# !pip install textblob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# # In case of any corpus are missing \n",
    "# download all-nltk\n",
    "#nltk.download()\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8237ab",
   "metadata": {},
   "source": [
    "PARTIE 1 : WORDCLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e3ac84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupère le texte d'un livre sans les mentions du projet Gutenberg\n",
    "def get_text_book(gutenberg_book):\n",
    "    #suppression de l'entête\n",
    "    if gutenberg_book.find(\"*** START\")!= -1:\n",
    "        end_head = gutenberg_book.find(\"*** START\")\n",
    "    elif  gutenberg_book.find(\"***START\")!= -1: \n",
    "        end_head = gutenberg_book.find(\"***START\")\n",
    "    else:\n",
    "        end_head = 1\n",
    "    book = gutenberg_book.replace(gutenberg_book[:end_head], \"\")\n",
    "    #Suppression du footer\n",
    "    if book.find(\"*** END\")!= -1:\n",
    "        start_footer = book.find(\"*** END\")\n",
    "    elif book.find(\"***END\")!= -1:\n",
    "        start_footer = book.find(\"***END\")\n",
    "    else:\n",
    "        start_footer = (len(book)-1)\n",
    "        #print(\"start_footer else: \" , start_footer)\n",
    "    book = book.replace(book[start_footer:], \"\")\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5c0b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operations de Cleaning \n",
    "def get_tokenized_book(book):\n",
    "    #passe tout en minuscule\n",
    "    book=book.lower()\n",
    "    #Supprime la ponctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    book = book.translate(translator)\n",
    "    #Supprime les espaces\n",
    "    book = \" \".join(book.split())\n",
    "    #suppression de tout ce qui n'est pas des lettres\n",
    "    book = re.sub('[^A-Za-z]', ' ', book)\n",
    "    #Supprime les mots vides\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.add(\"gutenberg\")\n",
    "    stop_words.add(\"gutenbergtm\")\n",
    "    stop_words.add(\"project\")\n",
    "    stop_words.add(\"ebook\")\n",
    "    #print(stop_words)\n",
    "        #Separe les mots \n",
    "    tokenized_book = word_tokenize(book)\n",
    "    filtered_text = [word for word in tokenized_book if word not in stop_words]\n",
    "    #Trouve la racine des mots\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = [stemmer.stem(word) for word in filtered_text]\n",
    "    #Verifie que la racine des mots appartient à la langue \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in stems]\n",
    "    cleaned_book=[word for word in lemmas if len(word)> 3]\n",
    "    return cleaned_book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4bebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#création wordcloud à partir d'un book nettoyé\n",
    "def get_wordcloud_by_txt(book_cleaned):\n",
    "    wordcloud = WordCloud(width = 1800,\n",
    "                          height = 1800,\n",
    "                          background_color = 'white',\n",
    "                          stopwords = stop_words,\n",
    "                          min_font_size = 10,\n",
    "                          max_words = 50).generate(book_cleaned)\n",
    "    return wordcloud\n",
    "\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis(\"off\")\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "221926a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retourne un bag of words à partir d'un livre tokenisé\n",
    "def get_bag_of_words(tokenized_book):\n",
    "    words_dict={}\n",
    "    text_book = \" \".join(tokenized_book)\n",
    "    for word in tokenized_book:\n",
    "        freq_word= text_book.count(word)/len(tokenized_book)\n",
    "        words_dict[word]=freq_word\n",
    "    bag_of_words = pd.DataFrame([words_dict], columns=words_dict.keys())\n",
    "    return bag_of_words.iloc[0].sort_values(ascending = False)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f8b5096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordcloud_by_bow(bag_of_words):\n",
    "    wordcloud = WordCloud(min_word_length = 3,background_color='white', max_words = 50)\n",
    "    wordcloud.generate_from_frequencies(bag_of_words)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "da37b803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre books_cleaned enregistrés:  52\n"
     ]
    }
   ],
   "source": [
    "#récupère la liste des 52 livres\n",
    "book_names = os.listdir('./data_books')\n",
    "#print(books)\n",
    "\n",
    "books_cleaned=[]\n",
    "\n",
    "#boucle sur la liste des livres\n",
    "for book_name in book_names:\n",
    "    f = open(\"data_books/\" + book_name, encoding='UTF-8' )\n",
    "    \n",
    "    #Récupère le texte d'un livre sans les mentions du projet Gutenberg\n",
    "    book=get_text_book(f.read())\n",
    "    \n",
    "    #Nettoyage et tokenization des textes\n",
    "    tokenized_book = get_tokenized_book(book)\n",
    "    \n",
    "    #Enregistrement du livre nettoyé\n",
    "    book_cleaned = \" \".join(tokenized_book)\n",
    "    books_cleaned.append(book_cleaned)\n",
    "    #Sauvegarde des books_cleaned\n",
    "    svg_book=open(\"books_cleaned/\"+book_name,\"w\",encoding=\"utf-8\")\n",
    "    svg_book.write(book_cleaned)\n",
    "    svg_book.close()\n",
    "    \n",
    "    #création des wordclouds à partir du text nettoyé\n",
    "    #wordcloud = get_wordcloud_by_txt(book_cleaned)\n",
    "    #wordcloud.to_file(\"img/\"+ book_name[0:-4] +\".png\")\n",
    "    \n",
    "    #création des bags_of_words\n",
    "    bag_of_words = get_bag_of_words(tokenized_book)\n",
    "    bag_of_words.to_csv(\"bags_of_words/\"+ book_name[0:-4] +\".csv\")\n",
    "    \n",
    "    #Création du wordcloud à partir du bag of words\n",
    "    wordcloud = get_wordcloud_by_bow(bag_of_words)\n",
    "    wordcloud.to_file(\"wordclouds/\"+ book_name[0:-4] +\".png\")\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "print(\"Nombre books_cleaned enregistrés: \" ,len(books_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1276e91",
   "metadata": {},
   "source": [
    "PARTIE 2 : TOPIC MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9b68bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre books_cleaned enregistrés final:  52\n"
     ]
    }
   ],
   "source": [
    "#récupère la liste des 52 livres\n",
    "import os\n",
    "book_names = os.listdir('./data_books')\n",
    "#print(books)\n",
    "\n",
    "books_cleaned=[]\n",
    "\n",
    "#boucle sur la liste des livres\n",
    "for book_name in book_names:\n",
    "    #if os.path.isdir(\"books_cleaned/\"+book_name)== False:\n",
    "    f = open(\"data_books/\" + book_name, encoding='UTF-8' )\n",
    "\n",
    "    #Récupère le texte d'un livre sans les mentions du projet Gutenberg\n",
    "    book=get_text_book(f.read())\n",
    "\n",
    "    #Nettoyage et tokenization des textes\n",
    "    tokenized_book = get_tokenized_book(book)\n",
    "\n",
    "    #Enregistrement du livre nettoyé\n",
    "    book_cleaned = \" \".join(tokenized_book)\n",
    "    books_cleaned.append(book_cleaned)\n",
    "\n",
    "    #print(\"Nombre books_cleaned enregistrés : \" ,len(books_cleaned))\n",
    "    f.close()\n",
    "    \n",
    "print(\"Nombre books_cleaned enregistrés final: \" ,len(books_cleaned))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8da8e5",
   "metadata": {},
   "source": [
    "---------- Paramètres d'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2487b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbr de caractéristique = nbr de mots\n",
    "n_features = 1000\n",
    "#nbr de topic /sujet à extraire\n",
    "n_components = 3\n",
    "#Etiquettes des topics pour model LSA\n",
    "lsa_topics = [\"Topic 1\", \"Topic 2\", \"Topic 3\"]\n",
    "#nbr de mots représentatifs sélectionnés pour l'analyse\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18a654b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création de la Matrix TF-IDF    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# Configuration d'un TF-IDF Vectorizer \n",
    "#supprime les mots qui apparaissent dans moins de 2 livres ou dans au moins 95% des livres\n",
    "#Le réglage ngram_rangesur (1, 2)indique que nous utilisons uni-gram et bi-gram lors de la vectorisation\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features = n_features,\n",
    "                                   stop_words = 'english',\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   use_idf = True) \n",
    " \n",
    "#Création de la matrix  \n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(books_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e744c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#création d'un dataframe pour afficher la Matrix\n",
    "document_term_matrix=pd.DataFrame(index = tfidf_vectorizer.get_feature_names())\n",
    "i=0\n",
    "for vector in tfidf_vectors:\n",
    "    document_term_matrix.insert( i , book_names[i] , vector.T.todense(), allow_duplicates=False)\n",
    "    i += 1\n",
    "document_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a86d9b3",
   "metadata": {},
   "source": [
    "PARTIE 3 : LATENTE SEMENTIC ANALYSIS (L.S.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7dea420a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='arpack', n_components=3, random_state=1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import random as sparse_random\n",
    "\n",
    "#Instanciation d'un analyseur LSA\n",
    "#random_state=1 permet d'ajuster l'instance une fois avec des valeurs prédéfinies\n",
    "#pour éviter le problème de sign indeterminnancy\n",
    "lsa = TruncatedSVD(n_components = n_components, random_state = 1, algorithm = 'arpack')\n",
    "\n",
    "#Ajustement / entrainement avec la matrix TF-IDF_vectors\n",
    "lsa.fit(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "33502e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retourne les topics d'après le model LSA\n",
    "def get_lsa_topics(lsa, vectorizer, topics, n_top_words = n_top_words):\n",
    "    word_dict = {}\n",
    "    #Récupère la liste des n mots de la Matrix\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    #Boucle sur les n topics\n",
    "    for topic_idx, topic in enumerate(lsa.components_):\n",
    "        #Récupère la listes des indices des n mots ayant les meilleurs values pour le topic concerné\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        #Récupère la listes des n mots grâce à leur indice\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        #Enregistre la liste des meilleurs mots pour chaque topic avec comme clef l'étiquette du topic\n",
    "        word_dict[topics[topic_idx]] = top_features\n",
    "\n",
    "    return pd.DataFrame(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "19f58f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>littl</td>\n",
       "      <td>acid</td>\n",
       "      <td>acid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>time</td>\n",
       "      <td>state</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>think</td>\n",
       "      <td>nation</td>\n",
       "      <td>solut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>unit state</td>\n",
       "      <td>carbon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>look</td>\n",
       "      <td>form</td>\n",
       "      <td>heat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>great</td>\n",
       "      <td>term</td>\n",
       "      <td>soap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>king</td>\n",
       "      <td>unit</td>\n",
       "      <td>substanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tell</td>\n",
       "      <td>copi</td>\n",
       "      <td>quantiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>good</td>\n",
       "      <td>water</td>\n",
       "      <td>boil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>thing</td>\n",
       "      <td>distribut</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic 1     Topic 2   Topic 3\n",
       "0   littl        acid      acid\n",
       "1    time       state     water\n",
       "2   think      nation     solut\n",
       "3    like  unit state    carbon\n",
       "4    look        form      heat\n",
       "5   great        term      soap\n",
       "6    king        unit  substanc\n",
       "7    tell        copi  quantiti\n",
       "8    good       water      boil\n",
       "9   thing   distribut      food"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nbr de mots représentatifs sélectionnés pour l'analyse\n",
    "n_top_words = 10\n",
    "top_words_by_topic = get_lsa_topics(lsa, tfidf_vectorizer, lsa_topics)\n",
    "top_words_by_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e397ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retourne le topic du texte passé en paramètre\n",
    "def get_inference(model, vectorizer, topics, text, threshold):\n",
    "    v_text = vectorizer.transform([text])\n",
    "    scores = model.transform(v_text)\n",
    "\n",
    "    labels = set()\n",
    "    for i in range(len(scores[0])):\n",
    "        if scores[0][i] > threshold:\n",
    "            labels.add(topics[i])\n",
    "\n",
    "    if not labels:\n",
    "        return 'None', -1, set()\n",
    "\n",
    "    return topics[np.argmax(scores)], scores, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "7bfaaaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANALYSE LSA\n",
    "\n",
    "#Récupèration des textes des livres nettoyés\n",
    "book_names = os.listdir('./books_cleaned')\n",
    "\n",
    "books_scores_dict={}\n",
    "\n",
    "#Déclaration des listes de livres par topic\n",
    "books_topic1=[]\n",
    "books_topic2=[]\n",
    "books_topic3=[]\n",
    "\n",
    "#boucle sur la liste des livres\n",
    "for i,book_name in enumerate(book_names):\n",
    "    #Récupération des résultats prédis pour chaque livre\n",
    "    book = open(\"books_cleaned/\" + book_name, encoding='UTF-8' ).read()\n",
    "    topic_predicted, scores, _ = get_inference(lsa, tfidf_vectorizer, lsa_topics, book, 0)\n",
    "    \n",
    "    #Enregistrement des scores de chaque livre\n",
    "    books_scores_dict[i+1]= scores[0]\n",
    "    \n",
    "    #Répartition des livres dans chaque topic\n",
    "    if topic_predicted == \"Topic 1\":\n",
    "        books_topic1.append(book_name)\n",
    "    elif topic_predicted == \"Topic 2\":\n",
    "        books_topic2.append(book_name)\n",
    "    elif topic_predicted == \"Topic 3\":\n",
    "        books_topic3.append(book_name)\n",
    "    else:\n",
    "        print(book_name, \"n'a pas été classé!\")\n",
    "\n",
    "#Création Dataframe reprenant les scores de chaque livre par Topic\n",
    "books_scores_by_topic = pd.DataFrame(books_scores_dict, index = lsa_topics).T\n",
    "books_scores_by_topic[\"Books Names\"]= book_names\n",
    "\n",
    "\n",
    "#Création des listes de livres classés par Topic\n",
    "list_books_topic1=pd.DataFrame(books_topic1, columns=[\"Topic 1\"])\n",
    "list_books_topic2=pd.DataFrame(books_topic2, columns=[\"Topic 2\"])\n",
    "list_books_topic3=pd.DataFrame(books_topic3, columns=[\"Topic 3\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "85db6cc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alice-in-wonderland.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Among the Forest People.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Curious Myths of the Middle Ages.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Democracy In America, Volume 1 (of 2).txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Histories of two hundred and fifty-one divisio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>History of King Charles The First of England.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How the Flag Became Old Glory.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Little Lord Fauntleroy.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Medieval People.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mother Storie.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Narrative and Critical History of America.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>O Pioneers.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Peter-pan.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Prince Prigio.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Complete Herbal.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Eighteenth Brumaire of Louis Bonaparte.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The fauna of the deep sea.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The Foundations of the Origin of Species.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The French Revolution.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The Greater Republic.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The History of England from the Accession of J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The History Of The Decline And Fall Of The Rom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Last Leaf.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The Magic of Oz.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The Natural Food of Man.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>The Princess and the Goblin.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Progress of Invention in the Nineteenth.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The Railway Children.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The Rose and the Ring.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The Ruins.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The Secret Garden.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>The Tale of Timmy Tiptoes.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>The Threefold Commonwealth.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>The United States of America Part I.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>The White Feather.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Three Minute Stories.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Through the Looking-Glass.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Tiger and Tom and Other Stories for Boys.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Tom Sawyer Abroad.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>War and Peace.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Topic 1\n",
       "0                             alice-in-wonderland.txt\n",
       "1                         Among the Forest People.txt\n",
       "2                Curious Myths of the Middle Ages.txt\n",
       "3           Democracy In America, Volume 1 (of 2).txt\n",
       "4   Histories of two hundred and fifty-one divisio...\n",
       "5    History of King Charles The First of England.txt\n",
       "6                   How the Flag Became Old Glory.txt\n",
       "7                          Little Lord Fauntleroy.txt\n",
       "8                                 Medieval People.txt\n",
       "9                                   Mother Storie.txt\n",
       "10      Narrative and Critical History of America.txt\n",
       "11                                     O Pioneers.txt\n",
       "12                                      Peter-pan.txt\n",
       "13                                  Prince Prigio.txt\n",
       "14                            The Complete Herbal.txt\n",
       "15     The Eighteenth Brumaire of Louis Bonaparte.txt\n",
       "16                      The fauna of the deep sea.txt\n",
       "17       The Foundations of the Origin of Species.txt\n",
       "18                          The French Revolution.txt\n",
       "19                           The Greater Republic.txt\n",
       "20  The History of England from the Accession of J...\n",
       "21  The History Of The Decline And Fall Of The Rom...\n",
       "22                                  The Last Leaf.txt\n",
       "23                                The Magic of Oz.txt\n",
       "24                        The Natural Food of Man.txt\n",
       "25                    The Princess and the Goblin.txt\n",
       "26    The Progress of Invention in the Nineteenth.txt\n",
       "27                           The Railway Children.txt\n",
       "28                          The Rose and the Ring.txt\n",
       "29                                      The Ruins.txt\n",
       "30                              The Secret Garden.txt\n",
       "31                      The Tale of Timmy Tiptoes.txt\n",
       "32                     The Threefold Commonwealth.txt\n",
       "33            The United States of America Part I.txt\n",
       "34                              The White Feather.txt\n",
       "35                           Three Minute Stories.txt\n",
       "36                      Through the Looking-Glass.txt\n",
       "37       Tiger and Tom and Other Stories for Boys.txt\n",
       "38                              Tom Sawyer Abroad.txt\n",
       "39                                  War and Peace.txt"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Affichages\n",
    "\n",
    "#books_scores_by_topic.reindex(columns = [\"Books Names\", \"Topic 1\", \"Topic 2\", \"Topic 3\"])\n",
    "\n",
    "list_books_topic1\n",
    "\n",
    "# list_books_topic2\n",
    "\n",
    "# list_books_topic3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681fa78",
   "metadata": {},
   "source": [
    "PARTIE 4 : DOCUMENT SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9343d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def best_recommended_books(book_name, nbr_book_reco):\n",
    "    list_recommended_book = []\n",
    "    \n",
    "    return list_recommended_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50522d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "nbr_book_reco = 5\n",
    "book_name = \"alice-in-wonderland\"\n",
    "\n",
    "list_recommended_book = best_recommended_books(book_name, nbr_book_reco)\n",
    "print(list_recommended_book)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
